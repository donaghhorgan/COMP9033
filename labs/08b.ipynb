{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP9033 - Data Analytics Lab 08b: Decision tree regression\n",
    "## Introduction\n",
    "\n",
    "This lab focuses on anomaly detection using decision tree and random forest regression. It's a direct counterpart to the linear regression modelling in Lab 06 and the $k$ nearest neighbours regression in Lab 07b. At the end of the lab, you should be able to use `scikit-learn` to:\n",
    "\n",
    "- Create a decision tree regression model and a random forest regression model.\n",
    "- Use the models to predict new values.\n",
    "- Measure the accuracy of the models.\n",
    "\n",
    "### Getting started\n",
    "\n",
    "Let's start by importing the packages we'll need. As usual, we'll import `pandas` for exploratory analysis, but this week we're also going to use the `tree` subpackage from `scikit-learn` to create decision tree models and the `ensemble` subpackage to create random forest models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "\n",
    "from math import sqrt # We'll need this later\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's load the data. Write the path to your `server_load_historical.csv` file in the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path = \"data/server_load_historical.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data we're loading is simulated network interface load data (in Gb/s) from a front end web server:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(path, parse_dates=True, index_col=\"time\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is noisy due to fluctuating user logins and data transfers and follows a seasonal trend:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(df.describe()) # Print summary statistics\n",
    "df.plot(); # Plot the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data modelling\n",
    "\n",
    "Let's build a decision tree regression model of the server load data. `scikit-learn` supports decision tree functionality via the [`tree`](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.tree) subpackage. This subpackage supports both decision tree regression and classification. We can use the [`DecisionTreeRegressor`](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html#sklearn.tree.DecisionTreeRegressor) class to build our model.\n",
    "\n",
    "As before, we want to use our existing knowledge of server load to predict how it will behave in the future. Specifically, we're interested in modelling the behaviour of the server load over time. Therefore, our predictor variable will be the time of day and our target variable will be the server load.\n",
    "\n",
    "Again, to get the time of day, we can subtract the first timestamp in our index from each timestamp in our index and convert the results to integer values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "time = (df.index - df.index[0]).astype('int') # Computes the time from midnight in nanoseconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we construct our feature matrix, $\\mathbf{X}$, using our predictor values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = pd.DataFrame({'time': time}, index=df.index)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we set our target variable, $\\mathbf{y}$, to be the server load data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = df['server_load']\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use hold out validation to estimate our model error. We can use the [`train_test_split`](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) method (just like last week's lab) to split the data into a training set and a test set. Let's use 20% of the data for testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision tree regression\n",
    "\n",
    "`DecisionTreeRegressor` accepts a number of different hyperparameters and the model we build may be more or less accurate depending on their values. We can get a list of these modelling parameters using the `get_params` method of the estimator (this works on any `scikit-learn` estimator), like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DecisionTreeRegressor().get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can find a more detailed description of each parameter in the `scikit-learn` [documentation](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html#sklearn.tree.DecisionTreeRegressor). \n",
    "\n",
    "Let's build our decision tree model using a single stage pipeline and a grid search. Note how, like $k$ nearest neighbours, we don't need to do any scaling or polynomial feature generation like we did with the linear regression model in Lab 06."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipeline = make_pipeline(\n",
    "    DecisionTreeRegressor(random_state=0) # Control randomness, so the lab works the same for everyone\n",
    ")\n",
    "\n",
    "parameters = {\n",
    "    'decisiontreeregressor__criterion': ['mse', 'mae'],\n",
    "    'decisiontreeregressor__min_samples_leaf': [1, 5],\n",
    "    'decisiontreeregressor__min_samples_split': [2, 5] # 2 is the minimum\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(pipeline, parameters, cv=5) # Use 5 fold cross validation when selecting the best model\n",
    "gs.fit(X_train, y_train); # Fit using the training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check how well our model works by using it to predict values for the test set and measuring the error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make predictions about the test data\n",
    "y_pred = gs.predict(X_test)\n",
    "\n",
    "# Print error measurementsa\n",
    "print('MAE: %.2f Gb/s' % mean_absolute_error(y_test, y_pred))\n",
    "print('RMSE: %.2f Gb/s' % sqrt(mean_squared_error(y_test, y_pred))) # Use sqrt to get the RMSE from the MSE\n",
    "\n",
    "# Plot the predictions\n",
    "ax = df.plot()\n",
    "\n",
    "predicted = pd.DataFrame({'predicted': y_pred}, index=X_test.index)\n",
    "predicted.plot(ax=ax, marker='o', linewidth=0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decision tree model fits the overall trend of the server load data, but there is quite a lot of variance from sample to sample. While we could build a better model using a more exhaustive grid search (try this for yourself), let's use random forest regression instead to see if we can make an improvement.\n",
    "\n",
    "### Random forest regression\n",
    "\n",
    "Decision tree regression works well in lots of cases, but tends to be unstable near decision boundaries. Let's use random forests to build an alternative regression model and see what the difference is.\n",
    "\n",
    "`scikit-learn` supports ensemble model functionality via the [`ensemble`](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.ensemble) subpackage. This subpackage supports random forest regression and classification, as well as several other ensemble modelling algorithms. We can use the [`RandomForestRegressor`](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html#sklearn.ensemble.RandomForestRegressor) class to build our model.\n",
    "\n",
    "When building random forest regressors, we can choose to specify the same hyperparameters as with decision tree regressors. Let's also specify a number of different forest sizes in our grid search. We can do this using the `n_estimators` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipeline = make_pipeline(\n",
    "    RandomForestRegressor(random_state=0)\n",
    ")\n",
    "\n",
    "parameters = {\n",
    "    'randomforestregressor__n_estimators': [2, 5, 10, 15, 20],\n",
    "    'randomforestregressor__criterion': ['mse', 'mae'],\n",
    "    'randomforestregressor__min_samples_leaf': [1, 5],\n",
    "    'randomforestregressor__min_samples_split': [2, 5] # 2 is the minimum\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(pipeline, parameters, cv=5)\n",
    "gs.fit(X_train, y_train); # Fit using the training set\n",
    "\n",
    "# Make predictions about the test data\n",
    "y_pred = gs.predict(X_test)\n",
    "\n",
    "# Print error measurementsa\n",
    "print('MAE: %.2f Gb/s' % mean_absolute_error(y_test, y_pred))\n",
    "print('RMSE: %.2f Gb/s' % sqrt(mean_squared_error(y_test, y_pred))) # Use sqrt to get the RMSE from the MSE\n",
    "\n",
    "# Plot the predictions\n",
    "ax = df.plot()\n",
    "\n",
    "predicted = pd.DataFrame({'predicted': y_pred}, index=X_test.index)\n",
    "predicted.plot(ax=ax, marker='o', linewidth=0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, the random forest model better fits the trend of the server load data and has significantly lower error than the decision tree regressor model. Let's use the random forest model to construct an anomaly detector to automatically flag unusual server load activity in the future.\n",
    "\n",
    "## Anomaly detection\n",
    "\n",
    "Now that we have an accurate model, we can use it to detect anomalies in new data. Let's load some data where the server experiences an unusual load for the time of day:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path = 'data/server_load_new.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df2 = pd.read_csv(path, parse_dates=True, index_col='time')\n",
    "df2.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can extract our predictor matrix and our target variable just like before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_new = pd.DataFrame({'time': (df2.index - df2.index[0]).astype('int')}, index=df2.index)\n",
    "y_new = df2['server_load']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the difference between the new data and the model by plotting like before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make predictions about the test data\n",
    "y_pred = gs.predict(X_new)\n",
    "\n",
    "# Plot the predictions\n",
    "ax = df2.plot()\n",
    "\n",
    "predicted = pd.DataFrame({'predicted': y_pred}, index=X_new.index)\n",
    "predicted.plot(ax=ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While it is easy to gauge the anomaly visually, ideally an automatic system would detect it for us. This way, we don't have to micro-manage systems; instead, we can just sit back and wait to be alerted. Let's use a standard score test to find observations that are extremely different from what we expect.\n",
    "\n",
    "The standard score test works by finding observations ($x_i$) in a sample ($X = \\{x_1, x_2, \\ldots, x_n\\}$) that are more than a given number ($\\lambda$) of standard deviations ($\\sigma_x$) away from the average value ($\\bar{x}$), i.e.\n",
    "\n",
    "\\begin{align}\n",
    "    \\left| z(x_i) \\right| > \\lambda,\n",
    "\\end{align}\n",
    "where\n",
    "\\begin{align}\n",
    "    z(x_i) = \\frac{x_i - \\bar{x}}{\\sigma_x}.\n",
    "\\end{align}\n",
    "\n",
    "We can combine the equations above to work out the range of non-outlying observations, i.e.\n",
    "\n",
    "\\begin{align}\n",
    "    x_i \\in [\\bar{x} - \\lambda \\sigma_x, \\bar{x} + \\lambda \\sigma_x].\n",
    "\\end{align}\n",
    "\n",
    "Consequently, any observation that lies outside this range can be considered an anomaly.\n",
    "\n",
    "In our case, we want to detect observations that deviate significantly from the predicted value given by our model. We can do this by checking for model prediction errors that lie outside the range of non-outlying prediction errors. Let's use $\\lambda = 3$ to find instances where the model prediction error is more than three standard deviations from the mean prediction error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The prediction error on the training set represents the \"normal\" error we should expect\n",
    "error = y_train - gs.predict(X_train)\n",
    "\n",
    "# Compute the range of non-outlying prediction errors\n",
    "l = 3 # l = lambda\n",
    "min_error = error.mean() - l * error.std()\n",
    "max_error = error.mean() + l * error.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we compute the prediction error on the new data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "error = y_new - gs.predict(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the output, we can see exactly where an automated system would have shown us that an anomaly was occurring:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ax = df2.plot()\n",
    "ax.fill_between(df2.index, y_pred + min_error, y_pred + max_error, color='green', alpha=0.5)\n",
    "ax.fill_between(df2.index, ax.get_ylim()[0], ax.get_ylim()[1], color='red', alpha=0.5,\n",
    "                where=(error < min_error) | (error > max_error));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, the system detects the server load anomaly that occurs after 18:00, but it also catches two other anomalies, one just before 06:00 and the other just after 12:00. In these cases, the anomaly detector has misclassified the data as anomalous (i.e. it has made a mistake). We can increase our value of $\\lambda$ to ensure that this doesn't happen, at the cost of detecting fewer genuine anomalies. Alternatively, we can accept that the error rate is small enough to be tolerable, and perhaps suppress alerts for transient anomalies with short lifespans."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
